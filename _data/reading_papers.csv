title,author,description,year,url
Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits,"Amirhosein Ghasemabadi, Di Niu","Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample ",2026,https://doi.org/10.48550/arXiv.2512.20578
Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits,"Amirhosein Ghasemabadi, Di Niu","Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample ",2026,https://doi.org/10.48550/arXiv.2512.20578
Skillful joint probabilistic weather forecasting from marginals,"Ferran Alet, Ilan Price, Andrew El-Kadi et al.","Machine learning (ML)-based weather models have rapidly risen to prominence due to their greater accuracy and speed than traditional forecasts based on numerical weather prediction (NWP), recently out",2025,https://doi.org/10.48550/arXiv.2506.10772
AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques,"Aman Raj, Ankit Shetgaonkar, Lakshit Arora et al.","Natural disasters, including earthquakes, wildfires and cyclones, bear a huge risk on human lives as well as infrastructure assets. An effective response to disaster depends on the ability to rapidly ",2025,https://doi.org/10.1109/COMPSAC65507.2025.00251
Attention Is All You Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar et al.",The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and d,2023,https://doi.org/10.48550/arXiv.1706.03762
GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,"Yanping Huang, Youlong Cheng, Ankur Bapna et al.","Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond",2019,https://doi.org/10.48550/arXiv.1811.06965
Pointer Networks,"Oriol Vinyals, Meire Fortunato, Navdeep Jaitly",We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems ,2017,https://doi.org/10.48550/arXiv.1506.03134
Neural Message Passing for Quantum Chemistry,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley et al.","Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invaria",2017,https://doi.org/10.48550/arXiv.1704.01212
A simple neural network module for relational reasoning,"Adam Santoro, David Raposo, David G. T. Barrett et al.","Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a ",2017,https://doi.org/10.48550/arXiv.1706.01427
AdaGAN: Boosting Generative Models,"Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet et al.","Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to tra",2017,https://doi.org/10.48550/arXiv.1701.02386
Order Matters: Sequence to sequence for sets,"Oriol Vinyals, Samy Bengio, Manjunath Kudlur",Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations c,2016,https://doi.org/10.48550/arXiv.1511.06391
Multi-Scale Context Aggregation by Dilated Convolutions,"Fisher Yu, Vladlen Koltun","State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image cla",2016,https://doi.org/10.48550/arXiv.1511.07122
Neural Machine Translation by Jointly Learning to Align and Translate,"Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neur",2016,https://doi.org/10.48550/arXiv.1409.0473
Identity Mappings in Deep Residual Networks,"Kaiming He, Xiangyu Zhang, Shaoqing Ren et al.","Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behin",2016,https://doi.org/10.48550/arXiv.1603.05027
Recurrent Neural Network Regularization,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals","We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, doe",2015,https://doi.org/10.48550/arXiv.1409.2329
Deep Residual Learning for Image Recognition,"Kaiming He, Xiangyu Zhang, Shaoqing Ren et al.",Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly re,2015,https://doi.org/10.48550/arXiv.1512.03385
ImageNet Classification with Deep Convolutional Neural Networks,"Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton",,2012,https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
Nested Learning: The Illusion of Deep Learning Architecture,"Ali Behrouz, Meisam Razaviyayn, Peilin Zhong et al.","Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance th",,
Learning Domain-Driven Design,Vlad Khononov,,,
