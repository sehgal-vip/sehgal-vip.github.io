<!doctype html><html lang=en-us data-theme=dark><head><script>(function(){const e=localStorage.getItem("vipulsehgal-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;document.documentElement.setAttribute("data-theme",e||(t?"dark":"light"))})()</script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Understanding Transformers: A First Principles Approach | Vipul Sehgal</title><meta name=description content="Breaking down the transformer architecture that powers modern AI, explained without jargon."><meta name=author content="Vipul Sehgal"><meta property="og:title" content="Understanding Transformers: A First Principles Approach"><meta property="og:description" content="Breaking down the transformer architecture that powers modern AI, explained without jargon."><meta property="og:type" content="article"><meta property="og:url" content="https://sehgal-vip.github.io/writing/understanding-transformers-a-first-principles-approach/"><meta property="og:image" content="https://sehgal-vip.github.io/images/og-default.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@vipulsehgal"><meta name=twitter:title content="Understanding Transformers: A First Principles Approach"><meta name=twitter:description content="Breaking down the transformer architecture that powers modern AI, explained without jargon."><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=JetBrains+Mono:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=/css/main.min.df8c247e05156371860c5870539c39ddb265fab8023bb763a1917ffda8c16eff.css><link rel=canonical href=https://sehgal-vip.github.io/writing/understanding-transformers-a-first-principles-approach/><link rel=icon href=/favicon.ico></head><body><header class=site-header><a href=https://sehgal-vip.github.io/ class=site-logo><span class=site-logo-avatar>VS</span>
<span class=site-logo-name>Vipul Sehgal<span class=accent>.</span></span></a><div class=header-actions><button class=theme-toggle aria-label="Toggle theme" title="Toggle theme (T)" onclick=ThemeManager.toggle()>
<svg class="icon-sun" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
<svg class="icon-moon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></button></div></header><main class=container><article><header class=post-header><div class=post-header-top><a href=https://sehgal-vip.github.io/writing/ class=back-button>← Back</a><div class=post-meta><span class=post-date>Jan 15, 2025</span>
<span class="post-tag ai">AI</span></div></div><h1 class=post-header-title>Understanding Transformers: A First Principles Approach</h1></header><div class=post-content><p>The transformer architecture, introduced in the 2017 paper &ldquo;Attention Is All You Need,&rdquo; revolutionized natural language processing. But what makes it so powerful?</p><h2 id=the-core-intuition>The Core Intuition</h2><p>Imagine you&rsquo;re reading a sentence:</p><blockquote><p>&ldquo;The cat sat on the mat because it was tired.&rdquo;</p></blockquote><p>When you read &ldquo;it,&rdquo; your brain automatically connects it to &ldquo;cat&rdquo; rather than &ldquo;mat.&rdquo; You&rsquo;re performing attention—focusing on the relevant context to understand meaning.</p><p>Transformers do something similar, but mathematically.</p><h2 id=why-previous-approaches-failed>Why Previous Approaches Failed</h2><p>Before transformers, we had:</p><ol><li><strong>RNNs (Recurrent Neural Networks)</strong>: Processed text sequentially, one word at a time. Slow and struggled with long-range dependencies.</li><li><strong>LSTMs</strong>: Better at remembering, but still sequential and computationally expensive.</li></ol><p>The problem? They processed text like reading through a keyhole—one word at a time, trying to remember what came before.</p><h2 id=the-transformer-insight>The Transformer Insight</h2><p>Transformers said: &ldquo;What if we look at everything at once?&rdquo;</p><p>Instead of processing sequentially, transformers:</p><ol><li>Look at all words in parallel</li><li>Calculate how much each word should &ldquo;attend&rdquo; to every other word</li><li>Use these attention scores to build rich representations</li></ol><p>This is the <strong>self-attention mechanism</strong>—the heart of transformers.</p><h2 id=self-attention-in-three-steps>Self-Attention in Three Steps</h2><h3 id=step-1-create-queries-keys-and-values>Step 1: Create Queries, Keys, and Values</h3><p>For each word, we create three vectors:</p><ul><li><strong>Query (Q)</strong>: &ldquo;What am I looking for?&rdquo;</li><li><strong>Key (K)</strong>: &ldquo;What do I contain?&rdquo;</li><li><strong>Value (V)</strong>: &ldquo;What do I offer?&rdquo;</li></ul><h3 id=step-2-calculate-attention-scores>Step 2: Calculate Attention Scores</h3><p>We match each query against all keys to get attention scores. High scores mean strong relevance.</p><h3 id=step-3-weighted-combination>Step 3: Weighted Combination</h3><p>We use these scores to create a weighted combination of values. Words with high attention contribute more.</p><h2 id=why-this-matters>Why This Matters</h2><p>This architecture enables:</p><ul><li><strong>Parallel processing</strong>: Train on GPUs efficiently</li><li><strong>Long-range dependencies</strong>: Connect distant words easily</li><li><strong>Transfer learning</strong>: Pre-train once, fine-tune for many tasks</li></ul><p>It&rsquo;s why we have GPT, BERT, and the current AI revolution.</p><hr><p><em>In the next post, we&rsquo;ll explore how attention patterns reveal what language models &ldquo;understand&rdquo; about language.</em></p></div></article><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Transformers: A First Principles Approach","datePublished":"2025-01-15T00:00:00Z","dateModified":"2026-01-17T21:57:50\u002b05:30","author":{"@type":"Person","name":"Vipul Sehgal"},"description":"Breaking down the transformer architecture that powers modern AI, explained without jargon.","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/sehgal-vip.github.io\/writing\/understanding-transformers-a-first-principles-approach\/"}}</script><nav class=breadcrumb-trail aria-label="You are here"><span class=breadcrumb-label>You are here</span><div class=breadcrumb-path><a href=https://sehgal-vip.github.io/ class="breadcrumb-item breadcrumb-link"><span class=breadcrumb-arrow>←</span> Home
</a><span class=breadcrumb-separator>/</span>
<a href=https://sehgal-vip.github.io/writing/ class="breadcrumb-item breadcrumb-link">Writing</a>
<span class=breadcrumb-separator>/</span>
<span class="breadcrumb-item breadcrumb-current" title="Understanding Transformers: A First Principles Approach">Understanding Transformers: A First …</span></div></nav><footer class=footer><span>© 2026 Vipul Sehgal</span>
<span class=footer-hint>Press <kbd>⌘</kbd><kbd>K</kbd> to navigate</span></footer></main><div class=command-palette-overlay id=command-palette-overlay><div class=command-palette role=dialog aria-modal=true aria-label="Command palette"><div class=command-input-wrapper><input type=text class=command-input id=command-input placeholder="Type a command or search..." autocomplete=off spellcheck=false></div><div class=command-results id=command-results></div><div class=command-footer><span><kbd>↑</kbd><kbd>↓</kbd> navigate</span>
<span><kbd>↵</kbd> select</span>
<span><kbd>esc</kbd> close</span>
<span><kbd>T</kbd> toggle theme</span></div></div></div><script id=search-index type=application/json>{"posts":[{"title":"\"Hello World: Why I Started This Blog\"","url":"\"https://sehgal-vip.github.io/writing/hello-world-why-i-started-this-blog/\"","date":"Jan 2025","tags":"[\"Philosophy\"]"},{"title":"\"Understanding Transformers: A First Principles Approach\"","url":"\"https://sehgal-vip.github.io/writing/understanding-transformers-a-first-principles-approach/\"","date":"Jan 2025","tags":"[\"AI\"]"},{"title":"\"Three Product Thinking Frameworks That Changed How I Build\"","url":"\"https://sehgal-vip.github.io/writing/three-product-thinking-frameworks-that-changed-how-i-build/\"","date":"Jan 2025","tags":"[\"Product Management\"]"}],"links":[{"name":"\"GitHub\"","url":"\"https://github.com/sehgal-vip\""},{"name":"\"Twitter\"","url":"\"https://twitter.com/vipulsehgal\""},{"name":"\"LinkedIn\"","url":"\"https://linkedin.com/in/vipulsehgal\""},{"name":"\"Substack\"","url":"\"https://curiocityloop.substack.com\""},{"name":"\"Instagram\"","url":"\"https://instagram.com/vipulsehgal\""}]}</script><script src=/js/theme.min.30cb734dfe903ca9447f448afe77a4844856b4ef9841d730b78a1c8528250b90.js></script><script src=/js/command-palette.min.3cc61472a04100739dbc178a81233c6f03f6dbbe75b4d2c5df1b6dc76ee3135f.js></script></body></html>